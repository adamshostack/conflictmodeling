# 2. What Can Go Wrong?

This question is at the heart of technology threat modeling, and our tools, such as attack trees, can be adopted to conflict.  In "[Transforming Tech with Diversity-friendly software](https://docs.google.com/presentation/d/1JB3bTbJvjEypKlPu1JKV20Oz9YlF5zRCl3vLIPdDTrA/edit#slide=id.g2073602466_0_140)" Jon Pincus presented an attack tree for harrassing a person.  This is a way of investigating the question of "what can go wrong."  His tree is:

![A threat tree for harrassment comprised of triggering, flooding and making them look bad](https://github.com/adamshostack/conflictmodeling/blob/master/images/Harrassment-attack-tree-by-Pincus.png)

My current model is that there's interpersonal attacks and attacks by
nations or other large groups, although nations use interpersonal
attacks, there are things, such as election interference, that are
uncommon in interpersonal conflict.

##  Unintented Platform Effects on User Behaviour
Unintended consecounces of platform structure and capabilities. The structure of an online/social platform has the ability to influnce the behaviour of it's users. An example of that is the echo chamber where users are only shown content which agrees to their sensibilities. This makes the users more suseptable to different attacks.

# Ways of thinking about the problem

There are a lot of ways of conceptualizing the threats in this space, and none are inherently better than others. This page currently includes:

1. Misinformation/disinformation
2. Image-centric analysis
3. Text and words
4. Fake content (which is broader than mis-information)
5. Bullying
6. An actor-centric approach

## Misinformation/Disinformation
A taxonomy of mis- and disinformation:

* Fabricated content
* Manipulated content
* Imposter content
* False content
* Misleading content
* False connection
* Satire or parody

## Image-centric analysis
Conflict over images seems to run eternal.  From Nazi flags to nipples, image content is a constant battlefield.  This list is to provoke thinking about the range of issues presented with images, not to suggest solutions.

A taxonomy of images
1. Clothing/lack thereof
     1. Full nudity with consent of the model
     2. breasts/breastfeeding/female-presenting breasts
     3. Lack of burqa or other covering
     4. Unclothed children
          a. In art (eg, Led Zeppelin Houses of the Holy album cover; Ruebens, The Holy Family with Saints Francis and Anne and the Infant Saint John the Baptist)
          b. In family photos
2. Symbols
     1. Swastikas (banned in Germany, offensive in other places, sacred to Hindus)
     2. Controversial (eg, Confederate flags)
     3. Other political symbols either banned or offensive, and also meaningful
     4. Consider appropriated symbols such as Pepe the Frog?  (Pepe was adoped as a symbol of the alt-right. When do you ban him? Note that https://en.wikipedia.org/wiki/Pepe_the_Frog shows that Apple has banned it, Google has not.)
3. Behavior/action
    1. Murder videos
    2. incitement to criminal acts/terror
    3. Pornography (filmed/photographed)
    4. Pornography (surreptitious/revenge)
4. Representations
    1. Racial stereotypes
    2. Religious stereotypes
    3. Religious text (Doormat with verses from the Koran) See [Modan19](https://www.cair.com/good_news_alert_cair_welcomes_amazon_s_removal_of_doormats_bath_mats_with_islamic_religious_text)
    3. Representations of people (the prophet Mohammed)
5. Legal issues
     1. Copyright violations
     2. Lese-Majeste

## Text and Words: The Pen Is Mightier Than The Sword
Words can be used for nearly any purpose, and considering the ways in which they can be used to hurt seems a nearly endless process.  Words can either carry hurtful meanings ("You should be killed") or harmful content ("evil.org/infectme.html", "malware.zip").

* Imitation/impersonation
* Confusion including misleading news articles
* Deception
* Overwhelming
* Cheating
* Abuse
* Threats
* Harassment


### AI Driven text
Quantity has its own qualities.  Being able to scale to a very high volume of fake text can enable an attacker to overwhelm a discussion, drive out human conversation with either spam or apparently relevant content. In combination with fake accounts, it can tap into mechanisms like social proof, and make it appear that many people hold the view that the AI is espousing.  Even with a single account, it can generate so much text that it's impossible for a person to parse or respond to it all.  This can be a [Gish Gallop](https://en.wikipedia.org/wiki/Gish_gallop) on steroids.

In their [release of the GPT-2 language module](https://blog.openai.com/better-language-models/) the OpenAI institute flags the following issues:
* Generate misleading news articles
* Impersonate others online
* Automate the production of abusive or faked content to post on social media
* Automate the production of spam/phishing content
(Also, note that sample 8 uses logic very similar to sophisticated fake news postings, drawing on skepticism, critical thinking, and new approaches to a complex problem.  And was generated by an AI)






## Fake content
    1. Fake Identities - used to conceal / spoof identities
    2. Fake Audiences - used to fake level of support and social validation
    3. False Facts - Confident assertion of false information
    4. False Narratives - the use of false information and arguments to influnce public/user opinion.
   
## Bullying

There's quite a lot on cyber-bullying and it would be helpful to have a catalog or at least pointers

## A taxonomy by actor

## #Interpersonal attacks
(This is very, very incomplete)

Attacks: race, religion, sex, threats, politics, trolling, revenge, harrassment, libel.
Attacks outside the US might also include lesse-Majeste, blasphemy, or political advocacy.

### Attacks by nation states

#### Fake news
There's work to mine by at least Kate Starbird and Craig Silverman.

#### Elections 
At Facebook's F8 Conference in 2018, Alex Stamos presented a taxomony:  

* Attacks on knowledge including amplification of fake messages
* Safety of elections: harrassment, account takeover, threats of violence against voters, candidates, election officials
* attempts to disenfranchise/reduce turnout
* attempts to confuse votres to vote for a different candidate

 
