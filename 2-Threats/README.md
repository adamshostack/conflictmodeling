# 2. What Can Go Wrong?

This question is at the heart of technology threat modeling, and our tools, such as attack trees, can be adopted to conflict.  In "[Transforming Tech with Diversity-friendly software](https://docs.google.com/presentation/d/1JB3bTbJvjEypKlPu1JKV20Oz9YlF5zRCl3vLIPdDTrA/edit#slide=id.g2073602466_0_140)" Jon Pincus presented an attack tree for harrassing a person.  This is a way of investigating the question of "what can go wrong."  His tree is:

![A threat tree for harrassment comprised of triggering, flooding and making them look bad](https://github.com/adamshostack/conflictmodeling/blob/master/images/Harrassment-attack-tree-by-Pincus.png)

My current model is that there's interpersonal attacks and attacks by
nations or other large groups, although nations use interpersonal
attacks, there are things, such as election interference, that are
uncommon in interpersonal conflict.

##  Unintented Platform Effects on User Behaviour
Unintended consecounces of platform structure and capabilities. The structure of an online/social platform has the ability to influnce the behaviour of it's users. An example of that is the echo chamber where users are only shown content which agrees to their sensibilities. This makes the users more suseptable to different attacks.

# A taxonomy of images
Conflict over images seems to run eternal.  From Nazi flags to nipples, image content is a constant battlefield.  This list is to provoke thinking about the range of issues presented with images, not to suggest solutions.

1. Clothing/lack thereof
     1. Full nudity with consent of the model
     2. breasts/breastfeeding/female-presenting breasts
     3. Lack of burqa or other covering
2. Symbols
     1. Swastikas (banned in Germany, offensive in other places, sacred to Hindus)
     2. Controversial (eg, Confederate flags)
     3. Other political symbols either banned or offensive, and also meaningful
     4. Consider appropriated symbols such as Pepe the Frog?  (Pepe was adoped as a symbol of the alt-right. When do you ban him? Note that https://en.wikipedia.org/wiki/Pepe_the_Frog shows that Apple has banned it, Google has not.)
3. Behavior/action
    1. Murder videos
    2. incitement to criminal acts/terror
    3. Pornography (filmed/photographed)
    4. Pornography (surreptitious/revenge)
4. Representations
    1. Racial stereotypes
    2. Religious stereotypes
    3. Religious text (Doormat with verses from the Koran) See [Modan19](https://www.cair.com/good_news_alert_cair_welcomes_amazon_s_removal_of_doormats_bath_mats_with_islamic_religious_text)
    3. Representations of people (the prophet Mohammed)
5. Legal issues
     1. Copyright violations
     2. Lese-Majeste


# Fake content
    1. Fake Identities - used to conceal / spoof identities
    2. Fake Audiences - used to fake level of support and social validation
    3. False Facts - Confident assertion of false information
    4. False Narratives - the use of false information and arguments to influnce public/user opinion.
   

# Fake text
In their [release of the GPT-2 language module](https://blog.openai.com/better-language-models/) the OpenAI institute flags the following issues:
* Generate misleading news articles
* Impersonate others online
* Automate the production of abusive or faked content to post on social media
* Automate the production of spam/phishing content
(Also, note that sample 8 uses logic very similar to sophisticated fake news postings, drawing on skepticism, critical thinking, and new approaches to a complex problem.  And was generated by an AI)


# A taxonomy by actor

## Interpersonal attacks
(This is very, very incomplete)

Attacks: race, religion, sex, threats, politics, trolling, revenge, harrassment, libel.
Attacks outside the US might also include lesse-Majeste, blasphemy, or political advocacy.

## Attacks by nation states

### Fake news
There's work to mine by at least Kate Starbird and Craig Silverman.

### Elections 
At Facebook's F8 Conference in 2018, Alex Stamos presented a taxomony:  

* Attacks on knowledge including amplification of fake messages
* Safety of elections: harrassment, account takeover, threats of violence against voters, candidates, election officials
* attempts to disenfranchise/reduce turnout
* attempts to confuse votres to vote for a different candidate

 
